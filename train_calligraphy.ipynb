{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715ef1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--ttf_path TTF_PATH] [--chara CHARA]\n",
      "                             [--save_path SAVE_PATH] [--img_size IMG_SIZE]\n",
      "                             [--chara_size CHARA_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/hensonchen/Library/Jupyter/runtime/kernel-c5d4fde8-5fcc-45f9-acd9-14890cc51eca.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hensonchen/Documents/pytorch-test/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# !python font2img.py --ttf_path ttf_folder --chara total_chn.txt --save_path save_folder --img_size 80 --chara_size 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb47ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    ttf_path = \"ttf_folder\"\n",
    "    # calli_path = \"calli_folder\"\n",
    "    chara = \"total_chn_traditional.txt\"\n",
    "    save_path = \"save_folder\"\n",
    "    img_size = 140\n",
    "    chara_size = 60\n",
    "    num_calli = 3000\n",
    "    urls = [(\"zhiyong\", \"http://163.20.160.14/~word/modules/myalbum/viewcat.php?num=\" + str(num_calli) + \"&cid=27\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da19ea9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3529765013.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    data_dir = args.ttf_path\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image,ImageDraw,ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "# import argparse\n",
    "from fontTools.ttLib import TTFont\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Obtaining characters from .ttf')\n",
    "# parser.add_argument('--ttf_path', type=str, default='../ttf_folder',help='ttf directory')\n",
    "# parser.add_argument('--chara', type=str, default='../chara.txt',help='characters')\n",
    "# parser.add_argument('--save_path', type=str, default='../save_folder',help='images directory')\n",
    "# parser.add_argument('--img_size', type=int, help='The size of generated images')\n",
    "# parser.add_argument('--chara_size', type=int, help='The size of generated characters')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "file_object = open(args.chara, encoding='utf-8')   \n",
    "try:\n",
    "    characters = file_object.read()\n",
    "finally:\n",
    "    file_object.close()\n",
    "\n",
    "\n",
    "def draw_single_char(ch, font, canvas_size, x_offset, y_offset):\n",
    "    img = Image.new(\"RGB\", (canvas_size, canvas_size), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((x_offset, y_offset), ch, (0, 0, 0), font=font)\n",
    "    return img\n",
    "\n",
    "def draw_example(ch, src_font, canvas_size, x_offset, y_offset):\n",
    "    src_img = draw_single_char(ch, src_font, canvas_size, x_offset, y_offset)\n",
    "    example_img = Image.new(\"RGB\", (canvas_size, canvas_size), (255, 255, 255))\n",
    "    example_img.paste(src_img, (0, 0))\n",
    "    return example_img\n",
    "\n",
    "data_dir = args.ttf_path\n",
    "data_root = pathlib.Path(data_dir)\n",
    "print(data_root)\n",
    "\n",
    "all_image_paths = list(data_root.glob('*.*'))  # *.ttf TTF\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "total_num = len(all_image_paths)\n",
    "print(total_num)\n",
    "\n",
    "seq = list()\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "\n",
    "def get_char_list_from_ttf(font_file):\n",
    "    f_obj = TTFont(font_file)\n",
    "    m_dict = f_obj.getBestCmap()\n",
    "\n",
    "    unicode_list = []\n",
    "    for key, uni in m_dict.items():\n",
    "        unicode_list.append(key)\n",
    "\n",
    "    char_list = [chr(ch_unicode) for ch_unicode in unicode_list]\n",
    "    return char_list\n",
    "\n",
    "for idx, (label, item) in enumerate(zip(range(len(all_image_paths)),all_image_paths)):\n",
    "    print(\"{} / {} \".format(idx, total_num), item)\n",
    "    src_font = ImageFont.truetype(item, size=args.chara_size)\n",
    "    font_name = item.split('/')[-1].split('.')[0]\n",
    "    chars = get_char_list_from_ttf(item)  #\n",
    "    img_cnt = 0\n",
    "    for (chara, cnt) in zip(characters, range(len(characters))):\n",
    "        img = draw_example(chara, src_font, args.img_size, (args.img_size-args.chara_size)/2, (args.img_size-args.chara_size)/2)\n",
    "        path_full = os.path.join(args.save_path, 'id_%d'%(label))\n",
    "        if not os.path.exists(path_full):\n",
    "            os.mkdir(path_full)\n",
    "        if np.sum(np.array(img) / 255.) < 18000:\n",
    "            img_cnt += 1\n",
    "            img.save(os.path.join(path_full, \"%05d.png\" % (cnt)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18242f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of word list from `characters`, with (key, value) = (character, position_count)\n",
    "chara_dict = {}\n",
    "for (chara, cnt) in zip(characters, range(len(characters))):\n",
    "    chara_dict[chara] = cnt\n",
    "\n",
    "def download_data_with_no_check(path_full, soup):\n",
    "    # if not os.path.isdir(args.calli_path):\n",
    "    #     os.mkdir(Config.RAW_DIR)\n",
    "    #     os.mkdir(Config.TRAIN_DIR)\n",
    "    #     os.mkdir(Config.VAL_DIR)\n",
    "    #     os.mkdir(Config.EVAL_DIR)\n",
    "\n",
    "  # get all raw image tags and save them to directory \"zhiyong\"\n",
    "    def has_title(tag):\n",
    "        return tag.name == \"img\" and tag.has_attr('title')\n",
    "    raw_img_tags = soup.find_all(has_title)\n",
    "    print(raw_img_tags)\n",
    "    for raw_img_tag in raw_img_tags:\n",
    "        if \"http\" in raw_img_tag.get('src'):\n",
    "            lnk = raw_img_tag.get('src')\n",
    "            title = raw_img_tag.get('title')\n",
    "            pos = chara_dict[title]\n",
    "            with open(os.path.join(path_full, \"%05d.gif\" % (pos)), \"wb\") as f:\n",
    "                f.write(requests.get(lnk).content)\n",
    "\n",
    "    # loop through the titles and remove files with name length > 5\n",
    "    for filename in os.listdir(path_full):\n",
    "        # here it's 5 because we include \".gif\"\n",
    "        if len(filename) > 9 and os.path.isfile(path_full+\"/\"+filename) :\n",
    "            f = os.path.join(path_full, filename)\n",
    "            os.remove(f)\n",
    "\n",
    "def download_data(path_full, soup):\n",
    "    if os.path.exists(path_full) and not os.path.isfile(path_full):\n",
    "        # Checking if the directory is empty or not\n",
    "        lstdir = os.listdir(path_full)\n",
    "        if not lstdir or (len(lstdir) < 3 and lstdir[0][0] == '.'): # empty dir\n",
    "            print(\"data dir emtpy, downloading data\")\n",
    "            download_data_with_no_check()\n",
    "        else:\n",
    "            print(\"data dir not empty, skipping download\")\n",
    "            pass\n",
    "    else:\n",
    "        print(\"data dir does not exist, creating dir and downloading data\")\n",
    "        os.mkdir(path_full)\n",
    "        # os.mkdir(Config.RAW_DIR)\n",
    "        # os.mkdir(Config.TRAIN_DIR)\n",
    "        # os.mkdir(Config.VAL_DIR)\n",
    "        # os.mkdir(Config.EVAL_DIR)\n",
    "        download_data_with_no_check(path_full, soup)\n",
    "\n",
    "# calli_dir = args.calli_path\n",
    "# calli_root = pathlib.Path(calli_dir)\n",
    "# print(\"calli_root:\", calli_root)\n",
    "\n",
    "all_calli_urls = args.urls\n",
    "total_num = len(all_calli_urls)\n",
    "print(total_num)\n",
    "\n",
    "for idx, (label, (calli_style, url)) in enumerate(zip(range(len(all_image_paths), len(all_image_paths) + len(all_calli_urls)), all_calli_urls)):\n",
    "    print(\"Download \\'{}\\'... {} / {} \".format(calli_style, idx, total_num), url)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    folder_name = 'id_%d'%(label)\n",
    "\n",
    "    path_full = os.path.join(args.save_path, folder_name)\n",
    "    if not os.path.exists(path_full):\n",
    "        os.mkdir(path_full)\n",
    "\n",
    "    download_data(path_full, soup)\n",
    "\n",
    "    # src_font = ImageFont.truetype(item, size=args.chara_size)\n",
    "    # font_name = item.split('/')[-1].split('.')[0]\n",
    "    # chars = get_char_list_from_ttf(item)  #\n",
    "    # img_cnt = 0\n",
    "    # for (chara, cnt) in zip(characters, range(len(characters))):\n",
    "    #     img = draw_example(chara, src_font, args.img_size, (args.img_size-args.chara_size)/2, (args.img_size-args.chara_size)/2)\n",
    "    #     path_full = os.path.join(args.save_path, 'id_%d'%(label))\n",
    "    #     if not os.path.exists(path_full):\n",
    "    #         os.mkdir(path_full)\n",
    "    #     if np.sum(np.array(img) / 255.) < 18000:\n",
    "    #         img_cnt += 1\n",
    "    #         img.save(os.path.join(path_full, \"%05d.png\" % (cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce3e0b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'blobfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m dist_util, logger\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_datasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_data\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresample\u001b[39;00m \u001b[39mimport\u001b[39;00m create_named_schedule_sampler\n",
      "File \u001b[0;32m~/Documents/pytorch-test/diff-font/Font-diff/utils/dist_util.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msocket\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mblobfile\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmpi4py\u001b[39;00m \u001b[39mimport\u001b[39;00m MPI\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mth\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'blobfile'"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "from utils import dist_util, logger\n",
    "from utils.image_datasets import load_data\n",
    "from utils.resample import create_named_schedule_sampler\n",
    "from utils.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    args_to_dict,\n",
    "    create_model_and_diffusion,\n",
    ")\n",
    "from utils.train_util import TrainLoop\n",
    "import torch as th\n",
    "from attrdict import AttrDict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--cfg_path', type=str, default='./cfg/train_cfg.yaml',\n",
    "                        help='config file path')\n",
    "    parser = parser.parse_args()\n",
    "    with open(parser.cfg_path, 'r') as f:\n",
    "        cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    cfg = AttrDict(create_cfg(cfg))\n",
    "    train_step = cfg.train_step\n",
    "    total_train_step = cfg.total_train_step\n",
    "    sty_encoder_path = cfg.sty_encoder_path\n",
    "    classifier_free = cfg.classifier_free\n",
    "    cfg.__delattr__('train_step')\n",
    "    cfg.__delattr__('total_train_step')\n",
    "    cfg.__delattr__('sty_encoder_path')\n",
    "    cfg.__delattr__('classifier_free')\n",
    "\n",
    "    dist_util.setup_dist()\n",
    "\n",
    "    model_save_dir = cfg.model_save_dir  \n",
    "\n",
    "    if not os.path.exists(model_save_dir):\n",
    "        os.mkdir(model_save_dir)\n",
    "\n",
    "    logger.configure(dir=model_save_dir, format_strs=['stdout', 'log', 'csv']) \n",
    "\n",
    "    logger.log(\"creating model and diffusion...\")\n",
    "    model, diffusion = create_model_and_diffusion(\n",
    "        **args_to_dict(cfg, model_and_diffusion_defaults().keys())\n",
    "    )\n",
    "    model.to(dist_util.dev())\n",
    "    schedule_sampler = create_named_schedule_sampler(cfg.schedule_sampler, diffusion)\n",
    "\n",
    "    ### sty_encoder\n",
    "    if not cfg.resume_checkpoint:\n",
    "        logger.log(\"loading pre-trained model...\")\n",
    "        checkpoint = th.load(sty_encoder_path, map_location='cpu')\n",
    "        tmp_dict = {}\n",
    "        for k, v in checkpoint.items():\n",
    "            if k in model.sty_encoder.state_dict():\n",
    "                tmp_dict[k] = v\n",
    "        model.sty_encoder.load_state_dict(tmp_dict)\n",
    "\n",
    "        if classifier_free:\n",
    "            raise ValueError(f\"required conditional trained model, please fill in the model path in 'resume_checkpoint'\")\n",
    "\n",
    "    # frozen sty_encoder\n",
    "    for p in model.sty_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "\n",
    "    logger.log(\"creating data loader...\")\n",
    "    data = load_data(\n",
    "        data_dir=cfg.data_dir,\n",
    "        batch_size=cfg.batch_size,\n",
    "        image_size=cfg.image_size,\n",
    "        stroke_path=cfg.stroke_path,\n",
    "        classifier_free=classifier_free,\n",
    "    )\n",
    "\n",
    "    logger.log(\"training...\")\n",
    "    TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=cfg.batch_size,\n",
    "        microbatch=cfg.microbatch,\n",
    "        lr=cfg.lr,\n",
    "        ema_rate=cfg.ema_rate,\n",
    "        log_interval=cfg.log_interval,\n",
    "        save_interval=cfg.save_interval,\n",
    "        train_step=train_step,\n",
    "        resume_checkpoint=cfg.resume_checkpoint,\n",
    "        use_fp16=cfg.use_fp16,\n",
    "        fp16_scale_growth=cfg.fp16_scale_growth,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        classifier_free=classifier_free,\n",
    "        total_train_step=total_train_step\n",
    "    ).run_loop()\n",
    "\n",
    "\n",
    "def create_cfg(cfg):\n",
    "    defaults = dict(\n",
    "        data_dir=\"\",\n",
    "        schedule_sampler=\"uniform\",\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.0,\n",
    "        lr_anneal_steps=0,\n",
    "        batch_size=1,\n",
    "        microbatch=-1,\n",
    "        ema_rate=\"0.9999\",\n",
    "        log_interval=250,\n",
    "        save_interval=20000,\n",
    "        resume_checkpoint=\"\",\n",
    "        use_fp16=False,\n",
    "        fp16_scale_growth=1e-3,\n",
    "        stroke_path=None,\n",
    "        attention_resolutions='40, 20, 10',\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    defaults.update(cfg)\n",
    "    return defaults\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
